&ensp;&ensp;&ensp;&ensp;终于弄好了，长舒一口气，经过几天的奋斗，终于把整个小东西克服了。既然是开篇，先介绍一下事情的原委：几周前自己通过fiddler分析某股票App上的数据接口，并根据自己的一些设想，利用这些接口爬取了少量数据进行了验证，发现效果良好。正当自己信心满满准备扩大数据集最更大范围内的验证时，发现后院失火了：之前整理出来的数据接口都拿不到数据了，注意是拿不到数据，而不是调不通。我将链接放到浏览器中打开，发现一点问题都没有，但是直接通过接口请求，返回的确实几条简单的js引用和一句window.location.href='xxxx链接'。
&ensp;&ensp;&ensp;&ensp;懂js的小伙伴，可能很多都想到了，这不就是多了一层重定向嘛，直接提取出window.location.href中的链接继续访问不就出来了。但是问题出在这个window.location.href里的链接，居然和我原本请求的链接一模一样。这特么就很诡异了，如果提取出来可以，那为什么直接请求不可以，所以肯定不是直接提取url再次请求这么简单。根据我们开发常识，很可能是那几个引入的js文件进行了什么不为人知的操作，于是我再次通过fiddler抓包发现，如果通过浏览器或者app里请求时，第二次请求连接时，cookie里是带着一个token的，而且每次都不一样，并且如果我装模作样随便传一个 token一样没有作用，那问题就出在某一个js文件里，利用一些算法得到了token再塞到请求里才能拿到参数。
&ensp;&ensp;&ensp;&ensp;所以爬虫进阶第一条路就显露出来了，分析js，研究里面的代码，然后自己来生成token，理论上是一定能实现的，但是成本不确定，并且一旦人家app哪天一高兴改了算法，你又要去研究。但是不管怎样，我还是去研究了，既然通过抓包分析道是操作cookie的，那我直接取每个js文件里搜，果然被我定位到了其中一个js文件里在进行setCookie的操作。但是，这文件长达几千行，生成cookie的算法真的是乱七八糟，看了两眼就想吐了。所以我向这个理论上绝对成立，实际上上手难度极高的方式妥协了。
&ensp;&ensp;&ensp;&ensp;第一种方式无法解决，那就只能第二种了selenium，还有其他的工具了，总体原理就是像浏览器一样或者直接操作浏览器去进行爬取，因为浏览器自己是会解析js的，所以这就是为什么上面那个网址浏览器可以打开，但是自己调接口去被反爬了的原因。这条进阶的道路的核心就是，执行js，面对成千上万行的js，既然无法分析，那就直接执行他们。就当我信心满满的一顿操作后，发现还是特么不行。直接炸毛了，一般大多数的爬虫到了这一步就无敌了，但是这次我还是倒下了。
&ensp;&ensp;&ensp;&ensp;分析js是不可能分析的，几千行代码鬼才去分析，直接借助selenium也被挡了，所以当时放弃了，准备去别家的app或者网站试试，结果发现有些数据，每家都不一样，而且数据跑下来还是他家的实验效果最好，期间还捐了钱用了一阵子tshare的接口，由于用的不是python语言（个人真的不建议使用Python。这种弱类型语言，写时一时爽，维护起来就妈卖批了，如果你是编程小白，看着教程就会点python那就算了，但是长远打算，python对项目的长期维护太烂了）还是用http方式调用获取的数据，发现也不好，有些数据还没有，白交了钱。
&ensp;&ensp;&ensp;&ensp;兜兜转转又回来了，走投无路。开始研究selenium被反爬的原理，后来发现是因为selenium启动浏览器，会在浏览器中注入一些特征，这些特征可以提取出来判断这不是正常的浏览器，基于这一原理，我对之前的那几个js文件进行了搜索，确实有这种特征判断。这时候就有两条路可走了，第一条是直接使用真实的浏览器，第二种是篡改js的逻辑，这两条路都可行，后面会细说，自己选择了第一条路，然后再window上通过直接启动浏览器，再用selenium进行控制，终于如偿所愿爬到了数据。这是我就想能不能放到阿里云上去，毕竟我不可能每天都去手动执行爬虫，还是的放到服务器上每天自动执行脚本才好，果然再次掉坑里了。
&ensp;&ensp;&ensp;&ensp;当我同样的方法在阿里云上配置后，发现阿里云无法爬取到数据，思来想去只可能有一个原因了：IP。可能将阿里云的网段都纳入到了黑名单中，为了解决这个问题，我只能通过代理带测试，最终我利用代理ip+谷歌浏览器+selenium成功在阿里云服务器上爬取到了我想要的数据。
&ensp;&ensp;&ensp;&ensp;整个爬虫的矛与盾就是，爬虫想法设法将自己显得像一个普通人在浏览数据，而反爬则是想方设法找到爬虫和普通人之间的区别，谁技高一筹，谁就赢了，理论上说，只要外网可访问的数据皆可爬。那有人会问，既然这些数据本来就是给人看的，那为什么要反爬虫呢，原因基于三点原因：1.不希望我的服务器收到爬虫的攻击，由于爬虫的高频访问会对应用服务器带来高并发，所以为了保证自己的稳定，所以有必要对爬虫进行反爬。2.我的数据你可以浏览，但是因为数据的高价值，我不允许你免费拿来做大数据分析。3.防止有人利用爬虫违反网络购物之类的公平性或者薅平台羊毛。比如利用爬虫脚本来淘宝抢单，完成拼多多每日任务等。

![爬虫进阶流程](..\picture_back_up\爬虫进阶流程.jpg)